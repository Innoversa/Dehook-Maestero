{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground for developing data preprocessing pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the personal dataset with Pandas\n",
    "\n",
    "`personal_fraud_email.csv` contains three columns:\n",
    "- `Sender`: String of the sender's name and email address\n",
    "- `Raw`: String of the raw email content\n",
    "- `Fraud`: Boolean of whether the email is a fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folder_path = \"fraud_emails\"\n",
    "csv_path = \"personal_fraud_email.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"ABC Shark Tank\" &lt;A@gaobie.fathetic.com&gt;</td>\n",
       "      <td>Received: from 10.217.135.43\\n by atlas108.fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"ABC Shark Tank\" &lt;A@herz.fathetic.com&gt;</td>\n",
       "      <td>Received: from 127.0.0.1\\n by atlas-production...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"ABC Shark Tank\" &lt;A@suipo.gangoulionectomy.com&gt;</td>\n",
       "      <td>Received: from 10.196.241.214\\n by atlas302.fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"%% Nicknguyen3 Camplejeunesuit %%\" &lt;utjnwaDhM...</td>\n",
       "      <td>Received: from 127.0.0.1\\n by atlas-production...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"PROGRESSIVE\" &lt;A@kudoke.iguanopy.com&gt;</td>\n",
       "      <td>Received: from 10.197.37.9\\n by atlas306.free....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sender  \\\n",
       "0           \"ABC Shark Tank\" <A@gaobie.fathetic.com>   \n",
       "1             \"ABC Shark Tank\" <A@herz.fathetic.com>   \n",
       "2    \"ABC Shark Tank\" <A@suipo.gangoulionectomy.com>   \n",
       "3  \"%% Nicknguyen3 Camplejeunesuit %%\" <utjnwaDhM...   \n",
       "4              \"PROGRESSIVE\" <A@kudoke.iguanopy.com>   \n",
       "\n",
       "                                                 Raw  Fraud  \n",
       "0  Received: from 10.217.135.43\\n by atlas108.fre...      1  \n",
       "1  Received: from 127.0.0.1\\n by atlas-production...      1  \n",
       "2  Received: from 10.196.241.214\\n by atlas302.fr...      1  \n",
       "3  Received: from 127.0.0.1\\n by atlas-production...      1  \n",
       "4  Received: from 10.197.37.9\\n by atlas306.free....      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Received: from 10.217.135.43\\n by atlas108.free.mail.ne1.yahoo.com with HTTPS; Fri, 23 Sep 2022 20:36:13 +0000\\nReceived: from 81.95.5.156 (EHLO gaobie.fathetic.com)\\n by 10.217.135.43 with SMTP;\\n Fri, 23 Sep 2022 20:36:13 +0000\\nFrom: \"ABC Shark Tank\" <A@gaobie.fathetic.com>\\nTo: <nicknguyen3@yahoo.com>\\nSubject: #1 Weight Loss Supplement ever\\nDate: Fri, 23 Sep 2022 15:36:13 -0500\\nMessage-ID: <724141433_848235704_895257853@gaobie.fathetic.com>\\nMIME-Version: 1.0\\nContent-Type: multipart/alternative;\\n\\tboundary=\"----=_NextPart_000_0AF5_01D8D2AF.71BEF780\"\\nX-Mailer: Microsoft Outlook 16.0\\nX-Originating-Ip: [81.95.5.156]\\nX-Originating-Ip: 81.95.5.156\\nAuthentication-Results: atlas108.free.mail.ne1.yahoo.com;\\n dkim=unknown;\\n spf=pass smtp.mailfrom=gaobie.fathetic.com;\\n dmarc=unknown header.from=gaobie.fathetic.com;\\nX-Apparently-To: nicknguyen3@yahoo.com; Fri, 23 Sep 2022 20:36:14 +0000\\nX-YMailISG: FDq_T_4WLDszG_9ocF2vWzkKzKZoMDdJEJDLnUIDgjBjN48W\\n jTYYWaWPjVmt1I.ylZ1R.b3aqC8rV2ij3F1Mdds5xP1AeyYOmS1vGg87LP0N\\n ZxZC8NX5Ga7g6GtC.dvFJ7tVzw7M0TjU0IBDTpAgndZUrOLwqlBIPGG5RXmG\\n J1uZUtuqEO5_HYqIDB47vDIOsARY4vtVGImffFqJFRJqM3S75qwcPSBcgZJz\\n _Vq22d3y0SvybmrOmH2ke27DLyPEqivFpU2xJMJJzlT0A_kNlw0EmLr4Trhc\\n u9qMQJd7BeV12TIspllYU5RxE5F6Cv22BwvIu3h8IPkXbm8gC5ZkqlMr7hwK\\n wGnJYcJ4RC_38Q.br.4Bs1sw.pbAtqarU8ZJEE6WuiBpozWEJ.Am1ezCz9Lg\\n o_v4T3TmWFIcV0YX5aD2VdlT9nbYeV3HWRNngwaMUyh55rIpzLl.adlEDCTT\\n yM2.Z4RMSX1c7rIoQQTy6IYnX_Pe3f3yLO4p79lmlFz0jnIVETN6dSUK88J_\\n FNbykLfQY5LhqPh8yBxJcdwOX9462f1dM55gWsWkYFOwUTcCJfx7DgppTKSw\\n s3P2u6wOkjeHxz6uCrQ1dvaJA4oCbtLtljoDZXVnTU1giGlC.1AP5EUqyAaS\\n A0lcPtFI2knmE7x7cdemiXMAXQlubRi24_h5c1ETbKvHBMhFQE4iKDA9jMyB\\n HJHKCzrKk3JnZ6guuiuHSyLaZYHMX2rWUd_RnQ_ikHAhPv3ZTnX_JzCAv2LZ\\n wHv773fW4PtJ8kuPkrybZdneOtsKhkuCANyU4u6ItKIQGhzZHH.XjgXwRA3B\\n x1B._6F1Ykx6wu1_KCPikNl5Xdpqtd7IF4xUOMc8vgglxfsXq0Oh4.x4CJ7K\\n mgTqrdb3AM5Gw19i_.kslBXr6tX1l03g3OvpmbpIIln8W7A1UgLsXVylrczb\\n p6OF\\n\\trKCMsdymV1W5Xp8IuQ.h7qti8mfjUzTS.dqqAxWcLsWg5_P10X.Iamuq\\n ms._3VSlFG75ZoMd_tGxI3GayJg35nNlrnND_q4Ae2O3yHW0mcS3WzhtOmCJ\\n EvcZiF81n0Pjm2qT1HeF4A0I5xuVvPrJOPy4Z8iU_yTPHPncjM1eLyU8zMXN\\n vFUsdRREjzkHe5UPdQrZcQY4x6e896hbQC8lu6DtjjJ4H_2QWet_t4j_9jEX\\n 7Ufv_RwO0Q--\\nThread-Index: AQG4L7q7Lrr/mtPeMSRR2n2N8gzSyg==\\nX-OlkEid: 00000000C845D6B803DDF243A071803957DEFC26070034AD76187591974DBF6C4F76951C873A01000100000000003C926B4CE2E4194EBF40FC36E06EB2760000000068EF00003AF73326F97A6E4A8822113B6BAD5949\\nContent-Length: 2611\\n\\nThis is a multipart message in MIME format.\\n\\n------=_NextPart_000_0AF5_01D8D2AF.71BEF780\\nContent-Type: text/plain;\\n\\tcharset=\"us-ascii\"\\nContent-Transfer-Encoding: 7bit\\n\\n \\n<http://efferench.com/sssswcirect.html?od=1syr632e13acce298_vl_Active16vl_\\n1481.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0a1MfO> \\n\\n\\n\\nShark Tank Weight Loss\\n\\n\\nNo Diet or Exercise AND Still Lose Weight \\n\\n\\n\\n#1 Weight Loss Supplement ever\\n\\n\\n\\nTry KETO Today!\\n\\n\\n\\n  <http://efferench.com/39/231/99917/MDeGbndjMx.jpg> \\n\\n \\n<http://efferench.com/sssswcirect.html?od=8syr632e13acce298_vl_Active16vl_\\n1481.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0r5djf> \\n\\n\\n\\n------=_NextPart_000_0AF5_01D8D2AF.71BEF780\\nContent-Type: text/html;\\n\\tcharset=\"us-ascii\"\\nContent-Transfer-Encoding: quoted-printable\\n\\n<html>\\n<body>\\n\\t<center>\\n\\t\\t<a =\\nhref=3D\"http://efferench.com/sssswcirect.html?od=3D1syr632e13acce298_vl_A=\\nctive16vl_1481.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0=\\na1MfO\" >\\n\\t\\t\\t\\t\\t\\t<h2><center>\\n<style>A {text-decoration: none;} </style>\\n<a href=3D\"\">\\n<h1 style=3D\"font-family: Segoe UI;font-weight: 100;color: =\\n#000;font-size: 35px;text-decoration: underline;\">\\nShark Tank Weight Loss</h1><h1 style=3D\"font: 59px Impact, Charcoal, =\\nsans-serif;color: #b9017a;margin: auto;\">\\nNo Diet or Exercise AND Still Lose Weight\\n <br>\\n <span style=3D\"color: #003e69;\">\\n </span>\\n</h1>\\n<h1 style=3D\"font-family: Segoe UI;font-size: 24px;margin: auto;color: =\\n#000;\">\\n#1 Weight Loss Supplement ever</h1>\\n<br>\\n<h1 style=3D\"font-family: calibri;color: #FFF;background-color: =\\n#b9017a;width: 300px;border-radius: 4px;border-bottom: 3px solid =\\n#6888B1;margin: auto;\">\\nTry KETO Today!</h1></div></a>\\n<br></h2>\\n\\t\\t</a>\\n\\t\\t<br />\\n\\t\\t<img src=3D\"http://efferench.com/39/231/99917/MDeGbndjMx.jpg\" =\\nheight=3D\"768\" width=3D\"725\" useMap=3D\"#Nzkf\" border=3D\"0\" >\\n\\t\\t<map name=3D\"Nzkf\"><br />\\n\\t\\t\\t<area shape=3D\"rect\" coords=3D\"0, 0, 725, 768\" =\\nhref=3D\"http://efferench.com/sssswcirect.html?od=3D1syr632e13acce298_vl_A=\\nctive16vl_1481.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0=\\na1MfO\" >\\n\\t\\t</map>\\n\\t\\t<br />\\n\\t\\t<img =\\nsrc=3D\"http://efferench.com/sssswcirect.html?od=3D8syr632e13acce298_vl_Ac=\\ntive16vl_1481.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0r=\\n5djf\" usemap=3D\"#MI9S\" border=3D\"0\" >\\n\\t\\t<map name=3D\"MI9S\">\\n\\t\\t\\t<area shape=3D\"rect\" coords=3D\"082,27,428,93\" =\\nhref=3D\"http://efferench.com/sssswcirect.html?od=3D1syr632e13acce298t0t_o=\\nutvl_Active16.5dopn4.O0000rgepb01kno021_xf1448.gepb0MDNxMzlnLTE4bHUxY3M0m=\\n4NPa\" >\\n\\t\\t</map>\\n\\t\\t<br />\\n\\t</center>\\n</body>\\n</html>\\n\\n------=_NextPart_000_0AF5_01D8D2AF.71BEF780--\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_df['Raw'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the Kaggle Dataset\n",
    "\n",
    "The `kaggle_fraud_email.csv` contains the two columns of emails collected from 1998 - 2007: \n",
    "- `Text`: String of the raw email content\n",
    "- `Class`: Boolean of whether the email is a fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kaggle_path = \"kaggle_fraud_email.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over. SidLet me know. Thx.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Friend,Greetings to you.I wish to accost ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not a surprising assessment from Embassy.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  Supply Quality China's EXCLUSIVE dimensions at...      1\n",
       "1                         over. SidLet me know. Thx.      0\n",
       "2  Dear Friend,Greetings to you.I wish to accost ...      1\n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....      1\n",
       "4          Not a surprising assessment from Embassy.      0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df = pd.read_csv(kaggle_path, encoding='utf-8')\n",
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_np = kaggle_df['Text'].astype(str).to_numpy()\n",
    "train_txt, test_txt = text_np[:int(len(text_np)*0.8)], text_np[int(len(text_np)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Supply Quality China's EXCLUSIVE dimensions at...\n",
       "1                               over. SidLet me know. Thx.\n",
       "2        Dear Friend,Greetings to you.I wish to accost ...\n",
       "3        MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....\n",
       "4                Not a surprising assessment from Embassy.\n",
       "                               ...                        \n",
       "11924                                                  NaN\n",
       "11925                                                  NaN\n",
       "11926                                                  NaN\n",
       "11927                                                  NaN\n",
       "11928                                                  NaN\n",
       "Name: Text, Length: 11929, dtype: object"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df['Text'] = kaggle_df['Text'].apply(lambda x: str(x) if isinstance(x, str) else x)\n",
    "kaggle_df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_np = []\n",
    "for text in kaggle_df['Text'].to_numpy():\n",
    "    if type(text) == str:\n",
    "        text_np.append(text)\n",
    "text_np = np.array(text_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:\n",
      " tensor([[ 101, 4425, 3737,  ..., 1997, 9812,  102],\n",
      "        [ 101, 2058, 1012,  ...,    0,    0,    0],\n",
      "        [ 101, 6203, 2767,  ..., 2009, 1010,  102],\n",
      "        ...,\n",
      "        [ 101, 2633, 1010,  ..., 6852, 1012,  102],\n",
      "        [ 101, 2720, 1027,  ..., 2052, 2022,  102],\n",
      "        [ 101, 1045, 1005,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the sentences\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in text_np:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode\n",
    "                        add_special_tokens = True,     # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,               # Pad & truncate all sentences.\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,  # Construct attn. masks.\n",
    "                        return_tensors = 'pt'          # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # Add its attention mask (differentiates padding from non-padding)\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Print the encoded sentences\n",
    "print(\"Input IDs:\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.1951e-02,  7.8548e-02, -2.3038e-02,  ..., -6.0618e-01,\n",
      "           4.0065e-01,  1.5508e-02],\n",
      "         [ 1.9590e-01, -1.1646e-01,  4.6568e-01,  ..., -2.8163e-01,\n",
      "           5.2920e-01,  1.0901e-01],\n",
      "         [ 6.6792e-02, -2.8796e-01,  5.9722e-01,  ..., -4.0285e-01,\n",
      "           2.3325e-01,  1.2778e-01],\n",
      "         ...,\n",
      "         [-5.4138e-01,  3.1125e-01,  2.9037e-01,  ..., -4.7552e-01,\n",
      "           3.7880e-02, -1.0177e+00],\n",
      "         [-1.8214e-01, -4.4945e-01, -2.9000e-01,  ..., -1.5307e-01,\n",
      "           5.5064e-01, -1.3873e+00],\n",
      "         [ 5.8956e-01,  7.3573e-02, -2.5281e-01,  ..., -3.0691e-01,\n",
      "          -4.1028e-01, -2.3539e-01]],\n",
      "\n",
      "        [[-2.6554e-01, -1.5594e-01,  6.3940e-01,  ..., -5.1358e-01,\n",
      "           2.8684e-01,  7.6536e-01],\n",
      "         [ 7.0034e-01, -2.2581e-01,  7.7793e-01,  ..., -3.8660e-01,\n",
      "           4.6591e-01,  6.0488e-01],\n",
      "         [-3.1319e-01, -6.6110e-01,  6.4022e-01,  ...,  2.1683e-02,\n",
      "           3.6389e-01,  3.0354e-01],\n",
      "         ...,\n",
      "         [ 2.1970e-01, -9.2227e-02,  8.3467e-01,  ..., -2.2059e-02,\n",
      "           1.0804e-02,  2.8839e-01],\n",
      "         [-8.1211e-02, -1.9978e-01,  9.9853e-01,  ..., -4.3842e-02,\n",
      "           6.4004e-02,  3.7402e-01],\n",
      "         [ 1.7763e-01, -1.8621e-01,  1.1277e+00,  ..., -2.8040e-01,\n",
      "           1.4281e-01,  6.1022e-01]],\n",
      "\n",
      "        [[-6.1453e-02, -9.2923e-02,  2.1703e-01,  ..., -2.4398e-01,\n",
      "           7.1162e-02,  6.3251e-01],\n",
      "         [ 5.4696e-01,  6.7749e-01,  5.7760e-01,  ..., -3.1697e-01,\n",
      "          -7.5292e-02,  1.8103e-02],\n",
      "         [ 4.7873e-01,  3.4855e-01,  1.0499e+00,  ...,  4.0860e-01,\n",
      "           2.9710e-01,  4.5172e-01],\n",
      "         ...,\n",
      "         [ 1.4574e-01, -6.4855e-01,  3.2064e-01,  ..., -2.3489e-01,\n",
      "          -1.2618e-02,  4.5878e-01],\n",
      "         [-1.7917e-01,  4.0322e-03, -2.2594e-01,  ...,  1.4485e-01,\n",
      "          -1.4672e-01, -5.8783e-02],\n",
      "         [ 3.7899e-01,  5.5510e-01,  1.1720e-01,  ...,  7.4089e-01,\n",
      "          -5.4155e-01, -1.7896e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7995e-01, -4.7720e-03,  1.2752e-01,  ..., -4.7056e-01,\n",
      "           5.9074e-01,  3.1194e-01],\n",
      "         [ 1.5080e-01, -2.0626e-01, -9.3308e-02,  ..., -2.7634e-02,\n",
      "           8.4430e-01,  9.8051e-01],\n",
      "         [ 3.8758e-01,  9.8123e-02,  1.1548e+00,  ...,  3.6277e-01,\n",
      "          -1.3391e-01,  3.9908e-01],\n",
      "         ...,\n",
      "         [ 1.0986e+00,  3.6397e-01,  8.3963e-01,  ...,  4.1552e-01,\n",
      "           2.8367e-01,  2.9656e-01],\n",
      "         [-3.0322e-01, -9.9300e-01, -5.5233e-01,  ...,  8.3714e-01,\n",
      "           4.6102e-01, -2.1624e-01],\n",
      "         [ 5.5527e-01,  4.6519e-01, -1.1239e-01,  ..., -1.2693e-01,\n",
      "          -6.6126e-01, -2.2800e-01]],\n",
      "\n",
      "        [[ 1.3627e-01,  3.1201e-01,  9.5576e-02,  ..., -1.7102e-01,\n",
      "           1.0002e-03,  4.2569e-01],\n",
      "         [-1.7620e-01,  2.7193e-01,  1.4091e+00,  ...,  1.8663e-01,\n",
      "          -1.4244e-02,  3.5002e-01],\n",
      "         [ 5.7884e-01, -1.3210e-01,  7.8871e-01,  ...,  5.5518e-02,\n",
      "          -2.1987e-02, -3.1932e-01],\n",
      "         ...,\n",
      "         [-1.7697e-01, -7.7085e-02,  9.6046e-01,  ...,  2.0311e-01,\n",
      "          -1.1236e-01,  5.8236e-02],\n",
      "         [ 3.3892e-02, -1.3262e-01,  3.2337e-01,  ...,  3.2227e-01,\n",
      "          -1.1860e-01,  4.6705e-02],\n",
      "         [-7.0266e-02, -4.3640e-02,  9.1084e-01,  ...,  1.6769e-01,\n",
      "          -7.3865e-02,  6.6925e-02]],\n",
      "\n",
      "        [[-3.3176e-01, -1.0756e-01,  2.8147e-02,  ..., -3.6815e-01,\n",
      "           2.3830e-01,  5.9297e-01],\n",
      "         [ 4.8321e-01, -9.4533e-01,  6.9124e-01,  ..., -3.0722e-02,\n",
      "           7.9239e-02,  2.8198e-01],\n",
      "         [ 6.5070e-01, -2.5370e-02,  7.4389e-01,  ...,  4.0832e-01,\n",
      "           2.8153e-01,  3.0229e-01],\n",
      "         ...,\n",
      "         [ 1.5077e-01, -4.3793e-01, -2.7747e-01,  ...,  6.5067e-01,\n",
      "          -1.1648e-01,  2.4495e-01],\n",
      "         [-6.0474e-01, -5.3556e-01, -8.6837e-01,  ...,  3.5429e-01,\n",
      "           4.7358e-02, -4.5471e-01],\n",
      "         [ 5.3965e-01,  3.2999e-01, -2.2079e-01,  ..., -5.7912e-02,\n",
      "          -6.9750e-01, -2.0575e-01]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Pass the input tensors through the BERT model\n",
    "outputs = model(input_ids=input_ids[:100, :], attention_mask=attention_masks[:100, :])\n",
    "\n",
    "# Print the final hidden states of the last layer of the model\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 64]), torch.Size([100, 64]), torch.Size([100, 64, 768]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[:100, :].shape, attention_masks[:100, :].shape, last_hidden_states.shape\n",
    "# Until this step, we are able to get the last hidden states"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing text\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "\n",
    "# define the tokenizer function\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# define the text field\n",
    "TEXT = torchtext.data.Field(tokenize=tokenize)\n",
    "\n",
    "# tokenize a text string\n",
    "# tokenized_text_list = []\n",
    "# for text in text_np:\n",
    "#     tokenized_text_list.append(TEXT.tokenize(text))\n",
    "#     print(len)\n",
    "\n",
    "# process the text sequences using the TEXT field\n",
    "TEXT.build_vocab(text_np)\n",
    "processed_text = TEXT.process(text_np)\n",
    "\n",
    "\n",
    "print ('finished processing text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77053, 11929])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text.shape\n",
    "torch.save(processed_text, 'processed_kaggle_text.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11929.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.434823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Class\n",
       "count  11929.000000\n",
       "mean       0.434823\n",
       "std        0.495754\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        1.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
