{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain a model that classifies fraudulent emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle_pt.shape: torch.Size([11928, 64, 768]), labels_pt.shape: torch.Size([11928])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "kaggle_pt = torch.load('kaggle_hidden_states.pt').float()\n",
    "labels_pt = torch.from_numpy(np.array(torch.load('kaggle_labels.pt'))).float()\n",
    "print(f\"kaggle_pt.shape: {kaggle_pt.shape}, labels_pt.shape: {labels_pt.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kaggle_pt` represents the kaggle hidden states observed by the pre-trained BERT model\n",
    "\n",
    "With the shape of [n, 64, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, dilation=2)\n",
    "        self.fc1 = nn.Linear(32 * 382, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool1d(x, kernel_size=2)  # Add max pooling layer\n",
    "        x = x.view(-1, 32 * 382)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc3(x)\n",
    "        output = self.sigmoid(output)\n",
    "        return output.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_device = \"cuda:3\"\n",
    "# define your model\n",
    "model = MLP().to(cuda_device)\n",
    "# define your loss function\n",
    "criterion = nn.BCELoss()\n",
    "# define your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle_pt[:32].shape, labels_pt[:32].shape, model(kaggle_pt[:32]).shape\n",
    "# loss = criterion(model(kaggle_pt[:32]), labels_pt[:32].float())\n",
    "# loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 0.2751535820524867, Val Loss: 0.21252139285206795\n",
      "Epoch: 2, Train Loss: 0.19493088962101354, Val Loss: 0.18338479846715927\n",
      "Epoch: 3, Train Loss: 0.16698520139950077, Val Loss: 0.16232554614543915\n",
      "Epoch: 4, Train Loss: 0.14697592869037535, Val Loss: 0.14718198776245117\n",
      "Epoch: 5, Train Loss: 0.13176632908786215, Val Loss: 0.13335424289107323\n",
      "Epoch: 6, Train Loss: 0.11886289425012542, Val Loss: 0.12301931902766228\n",
      "Epoch: 7, Train Loss: 0.10734970940322411, Val Loss: 0.11503180488944054\n",
      "Epoch: 8, Train Loss: 0.09875089947770281, Val Loss: 0.10624210350215435\n",
      "Epoch: 9, Train Loss: 0.0901965509827544, Val Loss: 0.1005475539714098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb Cell 7\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# train your model for 100 epochs\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m train(model, kaggle_pt, labels_pt, optimizer, criterion, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m)\n",
      "\u001b[1;32m/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# add retain_graph=True here\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bstmi1/home/grads/s/siconghuang/website/Dehook-Maestro/fraud_prediction.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def train(model, inputs, labels, optimizer, criterion, epochs, batch_size):\n",
    "    # split data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(inputs, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "    num_train_samples = X_train.shape[0]\n",
    "    num_val_samples = X_val.shape[0]\n",
    "\n",
    "    num_train_batches = num_train_samples // batch_size\n",
    "    num_val_batches = num_val_samples // batch_size\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # loop over training batches\n",
    "        for i in range(num_train_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "\n",
    "            # get the current batch\n",
    "            batch_inputs = X_train[start_idx:end_idx].to(cuda_device)\n",
    "            batch_labels = y_train[start_idx:end_idx].to(cuda_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward(retain_graph=True)  # add retain_graph=True here\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                train_loss += loss.item()\n",
    "\n",
    "        # loop over validation batches\n",
    "        for i in range(num_val_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "\n",
    "            # get the current batch\n",
    "            batch_inputs = X_val[start_idx:end_idx].to(cuda_device)\n",
    "            batch_labels = y_val[start_idx:end_idx].to(cuda_device)\n",
    "\n",
    "            # evaluate the model on validation set\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_loss /= num_train_batches\n",
    "        val_loss /= num_val_batches\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            # get the predictions on the training set and calculate f1 score\n",
    "            with torch.no_grad():\n",
    "                train_preds = model(X_train.to(cuda_device)).cpu().numpy()\n",
    "                train_f1 = f1_score(y_train.cpu().numpy(), (train_preds >= 0.5).astype(int))\n",
    "\n",
    "            # get the predictions on the validation set and calculate f1 score, auc roc, and auc pr\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_val.to(cuda_device)).cpu().numpy()\n",
    "                val_f1 = f1_score(y_val.cpu().numpy(), (val_preds >= 0.5).astype(int))\n",
    "                val_auc_roc = roc_auc_score(y_val.cpu().numpy(), val_preds)\n",
    "                val_auc_pr = average_precision_score(y_val.cpu().numpy(), val_preds)\n",
    "                    \n",
    "            print(f\"Epoch: {epoch+1}, Train BCE Loss: {train_loss}, Val BCE Loss: {val_loss}, F1 Score: {train_f1}/{val_f1}, AUC ROC: {val_auc_roc}, AUC PR: {val_auc_pr}\")\n",
    "\n",
    "\n",
    "\n",
    "# train your model for 100 epochs\n",
    "train(model, kaggle_pt, labels_pt, optimizer, criterion, epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11928, 64, 768]), (11928,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_pt.shape, labels_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11928])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pt[:-1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
